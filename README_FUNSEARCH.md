
# FunSearch Cache Replacement Policy Optimizer

This project implements a FunSearch-based system to automatically generate and optimize cache replacement policies for the ChampSim simulator.

## Overview

The FunSearch Cache Optimizer uses a combination of large language models (LLMs) and evolutionary principles to generate increasingly effective cache replacement policies. The system:

1. Generates candidate cache replacement policies using LLMs
2. Compiles these policies with ChampSim
3. Evaluates their performance using trace-driven simulation
4. Uses the best performers to guide the generation of new policies
5. Repeats the process over multiple generations to evolve increasingly effective policies

## Requirements

- Python 3.8+
- OpenAI API key (for GPT-4 or similar model access)
- ChampSim simulator installed and configured
- Trace files for cache simulation

## Setup

1. Install the required Python packages:
   ```
   pip install openai numpy
   ```

2. Clone and build ChampSim:
   ```
   git clone https://github.com/ChampSim/ChampSim.git
   cd ChampSim
   ```

3. Update the configuration in `funsearch_cache_optimizer.py`:
   - `CHAMPSIM_PATH`: Path to your ChampSim installation
   - `OPENAI_API_KEY`: Your OpenAI API key (or set it as an environment variable)
   - `TRACES_PATH`: Path to your ChampSim trace files
   - `OUTPUT_PATH`: Where to store generated policies and results

## Usage

Run the optimizer:

```
python funsearch_cache_optimizer.py
```

The script will:
1. Generate an initial population of cache replacement policies
2. Evaluate them against the reference DRRIP implementation
3. Select the best performers to generate new policies
4. Continue for the specified number of generations
5. Identify the best overall policy

## Configuration Options

You can modify these parameters in the script:

- `NUM_GENERATIONS`: Number of evolutionary generations to run
- `POPULATION_SIZE`: Number of policies in each generation
- `EVALUATION_CYCLES`: Number of simulation cycles for evaluation

## Output

The optimizer saves:
- Generated policy implementations (.cc files)
- Corresponding header files (.h files)
- Evaluation results for each policy
- Checkpoint files after each generation
- A final summary of the best policy and its performance

## How It Works

1. **Generation**: The LLM generates cache replacement policies based on provided context (initially the reference DRRIP policy, later the best performing policies)
2. **Compilation**: Policies are integrated with ChampSim and compiled
3. **Evaluation**: Policies are evaluated using trace-driven simulation, measuring MPKI (misses per kilo-instruction) and IPC (instructions per cycle)
4. **Selection**: The best performing policies are selected to serve as "parents" for the next generation
5. **Evolution**: New policies are generated by having the LLM "crossover" the best features of parent policies and introduce new optimizations

## Extending the Project

To extend this project:
- Add support for different LLM providers (Claude, Llama, etc.)
- Implement more sophisticated selection mechanisms
- Add parallel evaluation of policies
- Incorporate code analysis to guide the LLM's understanding of policy effectiveness
